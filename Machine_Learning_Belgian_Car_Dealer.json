{"paragraphs":[{"text":"%md\n.<center>![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  \n\n![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)</center>\n\n# Individual Assignment Big Data Tools 2\n\nIn this assignment you are going to consult a Belgian second hand car dealer. He has given you a set of cars that is available for sale on the market and is mainly interested in three things:\n- **Body styles** (SUV, 4x4, break, etc.):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* How many cars are available for each body style (desciptive)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* What is the average price of each car body style (desciptive)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*What are the features that distinguish between, body styles (ML)\n- **Age groups**:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* How many cars are available for each age group (desciptive)\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* What is the average price of each car age group (desciptive)\n- **Selling price**:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* What are the most important features that lead to a higher or lower price (ML)\n\nAs you can see, you need to perform some transformations, actions, desciptive statistics and machine learning. Can you help the Belgian car dealer by answering his questions?\n\n\n.<center>![Used Cars Logo](https://www.ocbc.com/assets/images/uploads/loans/inside_carloan/autofinancing_used_car.png)</center>\n","user":"anonymous","dateUpdated":"2017-11-15T18:39:09+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>.<center><img src=\"http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png\" alt=\"Spark Logo\" /> </p>\n<p><img src=\"http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png\" alt=\"Python Logo\" /></center></p>\n<h1>Individual Assignment Big Data Tools 2</h1>\n<p>In this assignment you are going to consult a Belgian second hand car dealer. He has given you a set of cars that is available for sale on the market and is mainly interested in three things:<br/>- <strong>Body styles</strong> (SUV, 4x4, break, etc.):<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* How many cars are available for each body style (desciptive)<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* What is the average price of each car body style (desciptive)<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*What are the features that distinguish between, body styles (ML)<br/>- <strong>Age groups</strong>:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* How many cars are available for each age group (desciptive)<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* What is the average price of each car age group (desciptive)<br/>- <strong>Selling price</strong>:<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* What are the most important features that lead to a higher or lower price (ML)</p>\n<p>As you can see, you need to perform some transformations, actions, desciptive statistics and machine learning. Can you help the Belgian car dealer by answering his questions?</p>\n<p>.<center><img src=\"https://www.ocbc.com/assets/images/uploads/loans/inside_carloan/autofinancing_used_car.png\" alt=\"Used Cars Logo\" /></center></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316868_1528893609","id":"20171029-125141_66295864","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T18:39:09+0000","dateFinished":"2017-11-15T18:39:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:341"},{"text":"%md\n## Practical\n\nGrading percentage: 35%\n\nDue Date: 15/12/2017\n\nSend assignment to: s.geuens@ieseg.fr\n\nAccepted formats: Zeppelin notebook (.json)\n\nIf you have finished the assignment, save this notebook pressing the \"Export this note\" icon on top in the middle of this page. The notebook will be saved as a .json file on your local machine. Rename this file as **lastname_firstname.json**. and send it to s.geuens@ieseg.fr.","user":"anonymous","dateUpdated":"2017-11-15T18:40:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Practical</h2>\n<p>Grading percentage: 35%</p>\n<p>Due Date: 15/12/2017</p>\n<p>Send assignment to: <a href=\"mailto:&#115;.&#103;&#x65;&#x75;&#x65;&#x6e;&#115;&#x40;&#x69;e&#115;&#x65;&#103;.&#102;&#114;\">&#115;.&#103;&#x65;&#x75;&#x65;&#x6e;&#115;&#x40;&#x69;e&#115;&#x65;&#103;.&#102;&#114;</a></p>\n<p>Accepted formats: Zeppelin notebook (.json)</p>\n<p>If you have finished the assignment, save this notebook pressing the &ldquo;Export this note&rdquo; icon on top in the middle of this page. The notebook will be saved as a .json file on your local machine. Rename this file as <strong>lastname_firstname.json</strong>. and send it to <a href=\"mailto:&#115;.&#103;e&#x75;&#101;&#x6e;&#x73;&#64;&#105;e&#x73;&#x65;g&#46;&#x66;&#x72;\">&#115;.&#103;e&#x75;&#101;&#x6e;&#x73;&#64;&#105;e&#x73;&#x65;g&#46;&#x66;&#x72;</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316868_1528893609","id":"20171030-081728_359422691","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T18:40:46+0000","dateFinished":"2017-11-15T18:40:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:342"},{"text":"%md\n## The assignment\n\n**The assignment consists of four major parts:**\n- Part 1: Putting the base file into HDFS\n- Part 2: Reading the file in Spark from HDFS\n- Part 3: Doing transformations and basic actions in Spark\n- Part 4: Calculating ML models in Spark\n- Part 5: Theoretical questions\n\nThis notebook is divided into these five parts. At the start of each part, an explanation of the expectations are given.\n\nThe assignment is grades based on logic and effort, not (only) on final results. If you make mistakes try to complete the steps that follow, even if you cannot complete them correctly anymore. Showing that you understand what you need to do, is as important as the final results. At the end of each block a test paragraph is included. This test paragraph already gives you an indication whether your answer is right or wrong.\n\n> As mentioned, grades are not only give based on final results. Try to be as complete as possible. Make notes by adding comments (#) or %md cells.","user":"anonymous","dateUpdated":"2017-11-15T18:40:59+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>The assignment</h2>\n<p><strong>The assignment consists of four major parts:</strong><br/>- Part 1: Putting the base file into HDFS<br/>- Part 2: Reading the file in Spark from HDFS<br/>- Part 3: Doing transformations and basic actions in Spark<br/>- Part 4: Calculating ML models in Spark<br/>- Part 5: Theoretical questions</p>\n<p>This notebook is divided into these five parts. At the start of each part, an explanation of the expectations are given.</p>\n<p>The assignment is grades based on logic and effort, not (only) on final results. If you make mistakes try to complete the steps that follow, even if you cannot complete them correctly anymore. Showing that you understand what you need to do, is as important as the final results. At the end of each block a test paragraph is included. This test paragraph already gives you an indication whether your answer is right or wrong.</p>\n<blockquote>\n  <p>As mentioned, grades are not only give based on final results. Try to be as complete as possible. Make notes by adding comments (#) or %md cells.</p>\n</blockquote>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316868_1528893609","id":"20171029-125559_1433266229","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-11T16:39:25+0000","dateFinished":"2017-11-11T16:39:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:343"},{"text":"%md\n## The Dataset\n**The dataset that is going to be used is the second_hand_cars.csv dataset containing 27412 cars and 9 columns (car features):**\n- brandName (informative)\n- modelName (informative)\n- bodyStyleName (feature/label)\n- cylinderCapcityValue (feature)\n- enginePowerValue (feature)\n- mileageValue (feature)\n- accident (filter)\n- age (feature)\n- sellingPrice (label/feature)","user":"anonymous","dateUpdated":"2017-11-15T19:40:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>The Dataset</h2>\n<p><strong>The dataset that is going to be used is the second_hand_cars.csv dataset containing 27412 cars and 9 columns (car features):</strong><br/>- brandName (informative)<br/>- modelName (informative)<br/>- bodyStyleName (feature/label)<br/>- cylinderCapcityValue (feature)<br/>- enginePowerValue (feature)<br/>- mileageValue (feature)<br/>- accident (filter)<br/>- age (feature)<br/>- sellingPrice (label/feature)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316868_1528893609","id":"20171029-125647_1176583291","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:40:37+0000","dateFinished":"2017-11-15T19:40:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:344"},{"text":"%md\n## Part 0: Loading Modules\n","dateUpdated":"2017-10-30T10:52:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 0: Loading Modules</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316868_1528893609","id":"20171029-193612_320959736","dateCreated":"2017-10-30T10:45:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:345"},{"text":"%pyspark\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import *\nfrom databricks_test_helper import Test\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator","user":"anonymous","dateUpdated":"2017-12-14T15:26:06+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316869_1528508860","id":"20171029-193638_1137130325","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:26:06+0000","dateFinished":"2017-12-14T15:26:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:346"},{"text":"%md\n## Part 1: Putting the base file into HDFS\n\nFirst task is to put the cars file into HDFS. Unfortunaltly, Zeppelin does not have a native HDFS interface, so to complete this first part, we need to use command line via PuTTY. Copy the commands you have run in the %md cell below.\n\n- make a new directory into HDFS called **/data/assignment**\n- add the file \"/home/bigdata//assignment/data/input/second_hand_cars.csv\" to HDFS in the directory created above","user":"anonymous","dateUpdated":"2017-11-15T19:41:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 1: Putting the base file into HDFS</h2>\n<p>First task is to put the cars file into HDFS. Unfortunaltly, Zeppelin does not have a native HDFS interface, so to complete this first part, we need to use command line via PuTTY. Copy the commands you have run in the %md cell below.</p>\n<ul>\n  <li>make a new directory into HDFS called <strong>/data/assignment</strong></li>\n  <li>add the file &ldquo;/home/bigdata//assignment/data/input/second_hand_cars.csv&rdquo; to HDFS in the directory created above</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316869_1528508860","id":"20171029-172647_992744864","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:41:50+0000","dateFinished":"2017-11-15T19:41:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:347"},{"text":"%md\n#### HDFS commands:\n\n#Start HDFS\nstart-dfs.sh\n\n#Listing files in HDFS\nhdfs dfs -ls /\n\n#Make a directory\nhdfs dfs -mkdir /data/assignment\n\n#write file to HDFS\nhdfs dfs -put /home/bigdata/assignment/data/input/second_hand_cars.csv  /data/assignment\n\n\n","user":"anonymous","dateUpdated":"2017-12-02T16:06:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>HDFS commands:</h4>\n<p>#Start HDFS<br/>start-dfs.sh</p>\n<p>#Listing files in HDFS<br/>hdfs dfs -ls /</p>\n<p>#Make a directory<br/>hdfs dfs -mkdir /data/assignment</p>\n<p>#write file to HDFS<br/>hdfs dfs -put /home/bigdata/assignment/data/input/second_hand_cars.csv /data/assignment</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316869_1528508860","id":"20171029-174250_1214746305","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-02T16:06:35+0000","dateFinished":"2017-12-02T16:06:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:348"},{"text":"%md\n## Part 2: Reading the file in Spark from HDFS\n\nBefore you are able to do some useful calculations, the dataset needs to be loaded into Spark. Read in the **second_hand_cars.csv** file you have put into HDFS in Part 1. The resulting dataframe should be called **raw_data**.\n\n> Note 1: Put hdfs://localhost:9000 infront of the location of the file in HDFS\n> Note 2: If you were not able to put the local file into HDFS, or not able to read the file from HDFS, you can read in the local file (\"/home/bigdata/assignment/data/input/second_hand_cars.csv\") directly.\n","user":"anonymous","dateUpdated":"2017-11-15T19:42:12+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 2: Reading the file in Spark from HDFS</h2>\n<p>Before you are able to do some useful calculations, the dataset needs to be loaded into Spark. Read in the <strong>second_hand_cars.csv</strong> file you have put into HDFS in Part 1. The resulting dataframe should be called <strong>raw_data</strong>.</p>\n<blockquote>\n  <p>Note 1: Put <a href=\"hdfs://localhost:9000\">hdfs://localhost:9000</a> infront of the location of the file in HDFS<br/>Note 2: If you were not able to put the local file into HDFS, or not able to read the file from HDFS, you can read in the local file (&ldquo;/home/bigdata/assignment/data/input/second_hand_cars.csv&rdquo;) directly.</p>\n</blockquote>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316869_1528508860","id":"20171029-180229_1491715068","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:42:12+0000","dateFinished":"2017-11-15T19:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:349"},{"text":"%pyspark\n# read in data\nraw_data = sqlContext.read.option(\"header\", True).csv(\"hdfs://localhost:9000/data/assignment/second_hand_cars.csv\")\nraw_data.show(5)","user":"anonymous","dateUpdated":"2017-12-14T15:26:33+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+---------+-------------+---------------------+----------------+------------+--------+------+------------+\n| brandName|modelName|bodyStyleName|cylinderCapacityValue|enginePowerValue|mileageValue|accident|   age|sellingPrice|\n+----------+---------+-------------+---------------------+----------------+------------+--------+------+------------+\n|      Audi|       Q2|          4x4|                 1598|              85|        7500|       0| 0.66 |       25990|\n|Volkswagen|   Tiguan|          4x4|                 1968|              81|       39000|       0| 3.41 |       20999|\n|      Ford|     Edge|          4x4|                 1997|             154|       34220|       0| 0.99 |       39900|\n|      Audi|       A1|      Berline|                 1422|              66|       14300|       0| 1.50 |       18990|\n|      Opel|    Corsa|      Berline|                 1398|              66|       19581|       0| 1.66 |       11489|\n+----------+---------+-------------+---------------------+----------------+------------+--------+------+------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1509360316869_1528508860","id":"20171029-180404_443768353","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:26:33+0000","dateFinished":"2017-12-14T15:26:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:350"},{"text":"%md\n## Part 3: Doing transformations and basic actions in Spark\n\nFirst, the car dealer is interested in some basic statistics of body styles and age groups. In this part some preprocessing is done, followed by the calculation of the desciptive statistics.\n\n- Cast string columns to float\n- Remove damaged cars & very old cars\n- Stats for bodyStyleName\n- Stats for age groups\n","user":"anonymous","dateUpdated":"2017-11-15T19:43:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 3: Doing transformations and basic actions in Spark</h2>\n<p>First, the car dealer is interested in some basic statistics of body styles and age groups. In this part some preprocessing is done, followed by the calculation of the desciptive statistics.</p>\n<ul>\n  <li>Cast string columns to float</li>\n  <li>Remove damaged cars &amp; very old cars</li>\n  <li>Stats for bodyStyleName</li>\n  <li>Stats for age groups</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316870_1529663106","id":"20171029-193853_819424861","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:43:04+0000","dateFinished":"2017-11-15T19:43:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:351"},{"text":"%md\n### (3a) Cast string columns to float\nCast the 6 last columns (cylinderCapacityValue - sellingPrice) to float. Do not forget to also include the real string columns in your dataframe.","user":"anonymous","dateUpdated":"2017-12-02T12:22:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(3a) Cast string columns to float</h3>\n<p>Cast the 6 last columns (cylinderCapacityValue - sellingPrice) to float. Do not forget to also include the real string columns in your dataframe.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316870_1529663106","id":"20171029-212410_1341566364","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-02T12:22:05+0000","dateFinished":"2017-12-02T12:22:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:352"},{"text":"%pyspark\n# Keep first 3 elements of string & cast numerical columns from string to float\nraw_data = raw_data.select(\"brandName\", \"modelName\", \"bodyStyleName\", *(col(c).cast(\"float\").alias(c) for c in raw_data.columns[-6:]))\n\nraw_data","user":"anonymous","dateUpdated":"2017-12-14T15:26:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DataFrame[brandName: string, modelName: string, bodyStyleName: string, cylinderCapacityValue: float, enginePowerValue: float, mileageValue: float, accident: float, age: float, sellingPrice: float]\n"}]},"apps":[],"jobName":"paragraph_1509360316870_1529663106","id":"20171029-190627_633320966","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:26:41+0000","dateFinished":"2017-12-14T15:26:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"text":"%pyspark\n# TEST\nTest.assertEquals(raw_data.count(), 27412, 'The dataframe does not have the correct number of rows.')\nTest.assertEquals(raw_data.columns, ['brandName', 'modelName', 'bodyStyleName', 'cylinderCapacityValue', 'enginePowerValue', 'mileageValue', 'accident', 'age', 'sellingPrice'], \"The columns are not correct.\")","user":"anonymous","dateUpdated":"2017-12-14T15:26:48+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316870_1529663106","id":"20171029-212715_1443590600","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:26:48+0000","dateFinished":"2017-12-14T15:26:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"text":"%md\n### (3b) Remove damaged cars & very old cars\n\nIn this part the cars that are damaged (accident = 1) need to be removed.\nAdditionally cars need to be between 0 and 95 years of age.","user":"anonymous","dateUpdated":"2017-11-15T19:44:15+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(3b) Remove damaged cars &amp; very old cars</h3>\n<p>In this part the cars that are damaged (accident = 1) need to be removed.<br/>Additionally cars need to be between 0 and 95 years of age.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316871_1529278358","id":"20171030-084251_1520910318","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:44:15+0000","dateFinished":"2017-11-15T19:44:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:355"},{"text":"%pyspark\n# Remove cars with accident = 1 and keep only the cars with age between 0 and 95\nfiltered_data = raw_data.filter('accident != 1').filter('age > 0').filter('age < 95')\nfiltered_data.count()","user":"anonymous","dateUpdated":"2017-12-14T15:27:03+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"27386\n"}]},"apps":[],"jobName":"paragraph_1509360316871_1529278358","id":"20171030-084302_264694410","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:26:58+0000","dateFinished":"2017-12-14T15:26:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:356"},{"text":"%pyspark\n# TEST\nTest.assertEquals(filtered_data.count(), 27386, 'The filtered_data dataframe does not have the correct number of rows.')","user":"anonymous","dateUpdated":"2017-12-14T15:27:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316871_1529278358","id":"20171030-084351_865275208","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:05+0000","dateFinished":"2017-12-14T15:27:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:357"},{"text":"%md\n> Note: If you were not able to create the filtered_data dataframe, you can read in the filtered_data file in /home/bigdata/assignment/data/input.","user":"anonymous","dateUpdated":"2017-11-15T19:45:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<blockquote>\n  <p>Note: If you were not able to create the filtered_data dataframe, you can read in the filtered_data file in /home/bigdata/assignment/data/input.</p>\n</blockquote>\n</div>"}]},"apps":[],"jobName":"paragraph_1509999061576_928701569","id":"20171106-201101_202654610","dateCreated":"2017-11-06T20:11:01+0000","dateStarted":"2017-11-15T19:45:06+0000","dateFinished":"2017-11-15T19:45:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:358"},{"text":"%md\n### (3c) Stats for bodyStyleName\n\nStarting from this part we can gather some useful results. Let us start with computing the descriptive statistics for **body style**.\n\n**Count** the number of cars per **bodyStyleName** and the **average sellingPrice** per **bodyStyleName**. Make sure the resulting dataframes are ordered in _descending_ order by count and average sellingPrice.\n","user":"anonymous","dateUpdated":"2017-11-15T19:45:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(3c) Stats for bodyStyleName</h3>\n<p>Starting from this part we can gather some useful results. Let us start with computing the descriptive statistics for <strong>body style</strong>.</p>\n<p><strong>Count</strong> the number of cars per <strong>bodyStyleName</strong> and the <strong>average sellingPrice</strong> per <strong>bodyStyleName</strong>. Make sure the resulting dataframes are ordered in <em>descending</em> order by count and average sellingPrice.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316871_1529278358","id":"20171029-212538_1508835095","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:45:38+0000","dateFinished":"2017-11-15T19:45:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:359"},{"text":"%pyspark\n# Count the number of cars per bodyStyleName and the average sellingPrice per bodyStyleName\n# Then sort them in descending order\ncountBodyStyleCars = filtered_data.groupBy(\"bodyStyleName\").count().orderBy(col(\"count\").desc())\npriceBodyStyleCars = filtered_data.groupBy(\"bodyStyleName\").mean(\"sellingPrice\").orderBy(col(\"avg(sellingPrice)\").desc())\n\ncountBodyStyleCars.show()\npriceBodyStyleCars.show()","user":"anonymous","dateUpdated":"2017-12-14T15:27:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-----+\n|       bodyStyleName|count|\n+--------------------+-----+\n|             Berline| 9860|\n|               Break| 4378|\n|                 4x4| 3925|\n|Monovolume/Camion...| 3194|\n|            Compacte| 2123|\n|           Cabriolet| 1483|\n|               Coupe| 1405|\n|                 Van|  931|\n|            Roadster|   46|\n|             Pick-Up|   41|\n+--------------------+-----+\n\n+--------------------+------------------+\n|       bodyStyleName| avg(sellingPrice)|\n+--------------------+------------------+\n|               Coupe|51982.614234875444|\n|            Roadster|40340.217391304344|\n|           Cabriolet| 34635.91975724882|\n|                 4x4|26008.535031847136|\n|             Pick-Up|23365.243902439026|\n|               Break| 19078.13042485153|\n|             Berline|15891.854462474645|\n|Monovolume/Camion...|14020.990607388854|\n|                 Van| 13778.63694951665|\n|            Compacte|12479.995760715969|\n+--------------------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1509360316871_1529278358","id":"20171029-193004_1869014331","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:10+0000","dateFinished":"2017-12-14T15:27:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:360"},{"text":"%pyspark\n# TEST\nTest.assertEquals(countBodyStyleCars.count(), 10, 'The countBodyStyleCars dataframe does not have the correct number of rows.')\nTest.assertEquals(countBodyStyleCars.select(\"count\").first()[\"count\"], 9860, \"The values in count column of countBodyStyleCars are not correct.\")\n\nTest.assertEquals(priceBodyStyleCars.count(), 10, 'The priceBodyStyleCars dataframe does not have the correct number of rows.')\nTest.assertEquals(round(priceBodyStyleCars.select(\"avg(sellingPrice)\").first()[\"avg(sellingPrice)\"], 2), 51982.61, \"The values in avg(sellingPrice) column of priceBodyStyleCars are not correct.\")","user":"anonymous","dateUpdated":"2017-12-14T15:27:16+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316871_1529278358","id":"20171029-213159_1212360536","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:16+0000","dateFinished":"2017-12-14T15:27:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:361"},{"text":"%md\n### (3d) Stats for age groups\n\nAfter calculating the statistics for body style, we can compute the same descriptives for **age group**.\n\nIn the first cell below you need to divide the **age** variable into 5 **age groups**.\n\nTo do this, create a Python function and load it into a udf. The Python function should create a string **age_group** column based on the **age** column with following values:\nage <= 3 years -> \"very young\"\nage > 3 years and age <= 7 -> \"young\"\nage > 7 years <= 12 -> \"medium\"\nage > 12 years <= 20 -> \"old\"\nage > 20 -> \"oldtimer\"\n\nIn the second cell you are going to calculate the **count** and average **sellingPrice** per **age_group**. Make sure the resulting dataframes are ordered in _descending_ order by count and average sellingPrice respectively.\n","user":"anonymous","dateUpdated":"2017-11-15T19:46:03+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(3d) Stats for age groups</h3>\n<p>After calculating the statistics for body style, we can compute the same descriptives for <strong>age group</strong>.</p>\n<p>In the first cell below you need to divide the <strong>age</strong> variable into 5 <strong>age groups</strong>.</p>\n<p>To do this, create a Python function and load it into a udf. The Python function should create a string <strong>age_group</strong> column based on the <strong>age</strong> column with following values:<br/>age &lt;= 3 years -&gt; &ldquo;very young&rdquo;<br/>age &gt; 3 years and age &lt;= 7 -&gt; &ldquo;young&rdquo;<br/>age &gt; 7 years &lt;= 12 -&gt; &ldquo;medium&rdquo;<br/>age &gt; 12 years &lt;= 20 -&gt; &ldquo;old&rdquo;<br/>age &gt; 20 -&gt; &ldquo;oldtimer&rdquo;</p>\n<p>In the second cell you are going to calculate the <strong>count</strong> and average <strong>sellingPrice</strong> per <strong>age_group</strong>. Make sure the resulting dataframes are ordered in <em>descending</em> order by count and average sellingPrice respectively.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316871_1529278358","id":"20171029-204255_2145846362","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:46:03+0000","dateFinished":"2017-11-15T19:46:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:362"},{"text":"%pyspark\n# Create Python function to transform age column into age_group\ndef ageTransform(age):\n    if age <= 3:\n        return \"very young\"\n    elif age > 3 and age <= 7:\n        return \"young\"\n    elif age > 7 and age <= 12:\n        return \"medium\"\n    elif age > 12 and age <= 20:\n        return \"old\"\n    elif age > 20:\n        return \"oldtimer\"\n\nudfAgeTransform = udf(ageTransform, StringType())\n\nfiltered_data = filtered_data.withColumn(\"age_group\", udfAgeTransform(\"age\"))","user":"anonymous","dateUpdated":"2017-12-14T15:27:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316872_1527354613","id":"20171029-211242_363191729","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:28+0000","dateFinished":"2017-12-14T15:27:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:363"},{"text":"%pyspark\n# Calculate count per age_group and sort by descending order\ncountAgeGroupCars = filtered_data.groupBy(\"age_group\").count().orderBy(col(\"count\").desc())\ncountAgeGroupCars.show()\n\n# Calculate average selling price per age_group and sort by descending order\npriceAgeGroupCars = filtered_data.groupBy(\"age_group\").mean(\"sellingPrice\").orderBy(col(\"avg(sellingPrice)\").desc())\npriceAgeGroupCars.show()","user":"anonymous","dateUpdated":"2017-12-14T15:27:30+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-----+\n| age_group|count|\n+----------+-----+\n|very young|10606|\n|     young|10248|\n|    medium| 4440|\n|       old| 1171|\n|  oldtimer|  921|\n+----------+-----+\n\n+----------+------------------+\n| age_group| avg(sellingPrice)|\n+----------+------------------+\n|  oldtimer| 44065.89576547231|\n|very young|25693.493211389778|\n|     young|17124.868169398906|\n|       old| 12019.67976088813|\n|    medium|11477.684684684684|\n+----------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1509360316872_1527354613","id":"20171029-214237_733133028","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:30+0000","dateFinished":"2017-12-14T15:27:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:364"},{"text":"%pyspark\n# TEST\nTest.assertEquals(countAgeGroupCars.count(), 5, 'The countAgeGroupCars dataframe does not have the correct number of rows.')\nTest.assertEquals(countAgeGroupCars.select(\"count\").first()[\"count\"], 10606, \"The values in count column of countAgeGroupCars are not correct.\")\n\nTest.assertEquals(priceAgeGroupCars.count(), 5, 'The priceAgeGroupCars dataframe does not have the correct number of rows.')\nTest.assertEquals(round(priceAgeGroupCars.select(\"avg(sellingPrice)\").first()[\"avg(sellingPrice)\"], 2), 44065.90, \"The values in avg(sellingPrice) column of priceAgeGroupCars are not correct.\")","user":"anonymous","dateUpdated":"2017-12-14T15:27:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316872_1527354613","id":"20171029-215326_845985810","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:36+0000","dateFinished":"2017-12-14T15:27:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:365"},{"text":"%md\n## Part 4: Calculating ML models in Spark\n\nIn this part we are going to answer the two machine learning based question:\n- What are the most important features that lead to a higher or lower price?\n- What are the features that distinguish between body styles?\n\nTo answer these questions, two types of models are going to be calculated; a linear regression and a logistic regression (not seen in class). You need to perform the following steps:\n- Transform bodyStyleName to index\n- Split data\n- Prepare data for linear regression\n- Initialize a linear regression model\n- Make predictions\n- Evaluate the linear regression model\n- Prepare data for logistic regression\n- Initialize a logistic regression model\n- Make predictions\n- Evaluate the logistic regression model\n","user":"anonymous","dateUpdated":"2017-12-02T12:14:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 4: Calculating ML models in Spark</h2>\n<p>In this part we are going to answer the two machine learning based question:<br/>- What are the most important features that lead to a higher or lower price?<br/>- What are the features that distinguish between body styles?</p>\n<p>To answer these questions, two types of models are going to be calculated; a linear regression and a logistic regression (not seen in class). You need to perform the following steps:<br/>- Transform bodyStyleName to index<br/>- Split data<br/>- Prepare data for linear regression<br/>- Initialize a linear regression model<br/>- Make predictions<br/>- Evaluate the linear regression model<br/>- Prepare data for logistic regression<br/>- Initialize a logistic regression model<br/>- Make predictions<br/>- Evaluate the logistic regression model</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316872_1527354613","id":"20171029-221132_1224904451","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-02T12:14:33+0000","dateFinished":"2017-12-02T12:14:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:366"},{"text":"%md\n### (4a) Transform bodyStyleName to index\ntransform **bodyStyleName** to **bodyStyleIndex** using the `StingIndexer()` method on the **filtered_data** dataframe.","user":"anonymous","dateUpdated":"2017-11-15T19:47:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4a) Transform bodyStyleName to index</h3>\n<p>transform <strong>bodyStyleName</strong> to <strong>bodyStyleIndex</strong> using the <code>StingIndexer()</code> method on the <strong>filtered_data</strong> dataframe.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316872_1527354613","id":"20171029-230637_1243346978","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:47:45+0000","dateFinished":"2017-11-15T19:47:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:367"},{"text":"%pyspark\n# Fit on filtered dataset to include bodyStyleName to index\nlabelIndexer = StringIndexer(inputCol=\"bodyStyleName\", outputCol=\"bodyStyleIndex\").fit(filtered_data)\n\n# Apply labelIndexer to model\nindexed_data = labelIndexer.transform(filtered_data)","user":"anonymous","dateUpdated":"2017-12-14T15:27:44+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316872_1527354613","id":"20171029-230740_41103202","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:44+0000","dateFinished":"2017-12-14T15:27:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:368"},{"text":"%pyspark\n# TEST\nTest.assertEquals(indexed_data.count(), 27386, 'The indexed_data dataframe does not have the correct number of rows.')\nTest.assertEquals(len(indexed_data.columns), 11, \"The indexed_data dataframe does not have the right number of columns.\")\nTest.assertEquals(indexed_data.columns[-1], \"bodyStyleIndex\", \"The final column of the indexed_data dataframe should be 'bodyStyleIndex'.\")","user":"anonymous","dateUpdated":"2017-12-14T15:27:47+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316872_1527354613","id":"20171030-085722_1636408343","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:47+0000","dateFinished":"2017-12-14T15:27:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:369"},{"text":"%md\n### (4b) Split data\n\nIn the labs in class, split data was done after preparing the data. Here the split step is shifted forward, because we only need to perform this step once, eventough two models are calculated.\n\nSplit the **indexed_data** dataframe into a **train_data**, **val_data**, and **test_data** with the follow split **[0.7, 0.2, 0.1]** and seed = **42**.","user":"anonymous","dateUpdated":"2017-11-15T19:48:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4b) Split data</h3>\n<p>In the labs in class, split data was done after preparing the data. Here the split step is shifted forward, because we only need to perform this step once, eventough two models are calculated.</p>\n<p>Split the <strong>indexed_data</strong> dataframe into a <strong>train_data</strong>, <strong>val_data</strong>, and <strong>test_data</strong> with the follow split <strong>[0.7, 0.2, 0.1]</strong> and seed = <strong>42</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316873_1526969864","id":"20171029-221604_1932967983","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:48:16+0000","dateFinished":"2017-11-15T19:48:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:370"},{"text":"%pyspark\n# Split data by stated weights and use seed = 42\n# train_data is used to build model\n# val_data & test_data are used to evaluated the trained (= fitted) model on unseen data\nweights = [.7, .2, .1]\nseed = 42\ntrain_data, val_data, test_data = indexed_data.randomSplit(weights, seed=seed)\n\n# Checking if the split is done correctly\nn_train = train_data.count()\nn_val = val_data.count()\nn_test = test_data.count()\n\nprint n_train, n_val, n_test, n_train + n_val + n_test\n","user":"anonymous","dateUpdated":"2017-12-14T15:27:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"19185 5460 2741 27386\n"}]},"apps":[],"jobName":"paragraph_1509360316873_1526969864","id":"20171029-222117_230396357","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:51+0000","dateFinished":"2017-12-14T15:27:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:371"},{"text":"%pyspark\n# TEST\nTest.assertEquals(train_data.count(), 19185, 'The train_data dataframe does not have the correct number of rows.')\nTest.assertEquals(val_data.count(), 5460, 'The val_data dataframe does not have the correct number of rows.')\nTest.assertEquals(test_data.count(), 2741, 'The test_data dataframe does not have the correct number of rows.')","user":"anonymous","dateUpdated":"2017-12-14T15:27:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316873_1526969864","id":"20171029-222220_1431642611","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:27:56+0000","dateFinished":"2017-12-14T15:27:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:372"},{"text":"%md\n### (4c) Prepare data for linear regression\nIn this part you need to prepare the **train_data**, **val_data**, and **test_data** by setting a **featureCol**.\n\nIn the linear regression you need to predict the **sellingPrice** based on the features bodyStyleName, cylinderCapacityValue, enginePowerValue, mileageValue, and age. You need to combine **bodyStyleIndex**, cylinderCapacityValue, enginePowerValue, mileageValue, and age into one featureCol called **features**. This needs to be done for the train_data, val_data, and test_data dataframes.","user":"anonymous","dateUpdated":"2017-11-15T19:48:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4c) Prepare data for linear regression</h3>\n<p>In this part you need to prepare the <strong>train_data</strong>, <strong>val_data</strong>, and <strong>test_data</strong> by setting a <strong>featureCol</strong>.</p>\n<p>In the linear regression you need to predict the <strong>sellingPrice</strong> based on the features bodyStyleName, cylinderCapacityValue, enginePowerValue, mileageValue, and age. You need to combine <strong>bodyStyleIndex</strong>, cylinderCapacityValue, enginePowerValue, mileageValue, and age into one featureCol called <strong>features</strong>. This needs to be done for the train_data, val_data, and test_data dataframes.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316873_1526969864","id":"20171029-222441_12328688","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:48:56+0000","dateFinished":"2017-11-15T19:48:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:373"},{"text":"%pyspark\ntrain_data_featured = (VectorAssembler(inputCols= [\"bodyStyleIndex\", \"cylinderCapacityValue\", \"enginePowerValue\", \"mileageValue\", \"age\"], outputCol=\"features\")\n                    .transform(train_data))\n\nval_data_featured =  (VectorAssembler(inputCols= [\"bodyStyleIndex\", \"cylinderCapacityValue\", \"enginePowerValue\", \"mileageValue\", \"age\"], outputCol=\"features\")\n                    .transform(val_data))\n\n\ntest_data_featured = (VectorAssembler(inputCols= [\"bodyStyleIndex\", \"cylinderCapacityValue\", \"enginePowerValue\", \"mileageValue\", \"age\"], outputCol=\"features\")\n                    .transform(test_data))\n","user":"anonymous","dateUpdated":"2017-12-14T15:28:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316873_1526969864","id":"20171029-222820_1494943918","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:01+0000","dateFinished":"2017-12-14T15:28:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:374"},{"text":"%pyspark\n# TEST\nTest.assertEquals(train_data_featured.columns[-1], \"features\", \"The final column of the train_data_featured dataframe should be 'features'.\")\nTest.assertEquals(val_data_featured.columns[-1], \"features\", \"The final column of the val_data_featured dataframe should be 'features'.\")\nTest.assertEquals(test_data_featured.columns[-1], \"features\", \"The final column of the test_dat_featured dataframe should be 'features'.\")\n\nTest.assertEquals(len(train_data_featured.select(\"features\").first()[\"features\"]), 5, \"The feature colomn in the train_data_featured dataframe should be a vector of 5 columns.\")\nTest.assertEquals(len(val_data_featured.select(\"features\").first()[\"features\"]), 5, \"The feature colomn in the val_data_featured dataframe should be a vector of 5 columns.\")\nTest.assertEquals(len(test_data_featured.select(\"features\").first()[\"features\"]), 5, \"The feature colomn in the test_data_featured dataframe should be a vector of 5 columns.\")","user":"anonymous","dateUpdated":"2017-12-14T15:28:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316873_1526969864","id":"20171030-090330_825978180","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:04+0000","dateFinished":"2017-12-14T15:28:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:375"},{"text":"%md\n### (4d) Initialize a linear regression model\n\nInitialize a **LinearRegression** with labelCol = **sellingPrice** and featuresCol = **features**. Afterwards, **fit** the model on the **train_data_featured** dataframe.","user":"anonymous","dateUpdated":"2017-11-15T19:49:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4d) Initialize a linear regression model</h3>\n<p>Initialize a <strong>LinearRegression</strong> with labelCol = <strong>sellingPrice</strong> and featuresCol = <strong>features</strong>. Afterwards, <strong>fit</strong> the model on the <strong>train_data_featured</strong> dataframe.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316873_1526969864","id":"20171030-090901_1711193824","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:49:30+0000","dateFinished":"2017-11-15T19:49:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:376"},{"text":"%pyspark\nlr = LinearRegression(labelCol=\"sellingPrice\", featuresCol=\"features\")\n\nmodel = lr.fit(train_data_featured)","user":"anonymous","dateUpdated":"2017-12-14T15:28:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316874_1528124111","id":"20171029-230335_1740517425","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:09+0000","dateFinished":"2017-12-14T15:28:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:377"},{"text":"%pyspark\n#TEST\nTest.assertEquals(str(model).split(\"_\")[0], 'LinearRegression', \"The model should be a LinearRegression object.\")","user":"anonymous","dateUpdated":"2017-12-14T15:28:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316874_1528124111","id":"20171030-091148_858191293","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:13+0000","dateFinished":"2017-12-14T15:28:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:378"},{"text":"%md\n### (4e) Make predictions\n\nMake predictions for the **train_data_featured**, **val_data_featured**, and **test_data_featured** dataframes.","user":"anonymous","dateUpdated":"2017-11-15T19:50:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4e) Make predictions</h3>\n<p>Make predictions for the <strong>train_data_featured</strong>, <strong>val_data_featured</strong>, and <strong>test_data_featured</strong> dataframes.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316874_1528124111","id":"20171030-091410_1863341565","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:50:36+0000","dateFinished":"2017-11-15T19:50:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:379"},{"text":"%pyspark\n# Make predictions\ntrain_predictions = model.transform(train_data_featured)\nval_predictions = model.transform(val_data_featured)\ntest_predictions = model.transform(test_data_featured)","user":"anonymous","dateUpdated":"2017-12-14T15:28:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316874_1528124111","id":"20171029-231544_1698015478","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:17+0000","dateFinished":"2017-12-14T15:28:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:380"},{"text":"%pyspark\n# TEST\nTest.assertEquals(train_predictions.columns[-1], \"prediction\", \"The final column of the train_predictions dataframe should be 'prediction'.\")\nTest.assertEquals(val_predictions.columns[-1], \"prediction\", \"The final column of the val_predictions dataframe should be 'prediction'.\")\nTest.assertEquals(test_predictions.columns[-1], \"prediction\", \"The final column of the test_predictions dataframe should be 'features'.\")","user":"anonymous","dateUpdated":"2017-12-14T15:28:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316874_1528124111","id":"20171030-091541_1019754387","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:20+0000","dateFinished":"2017-12-14T15:28:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:381"},{"text":"%md\n### (4f) Evaluate the linear regression\nIn the first cell, you need to evaluate the linear regression model by using the **RegressionEvaluator**. Calculate the **r2** and **mae** for the **train_data_featured**, **val_data_featured**, and **test_data_featured** dataframes.\n\nIn the second cell, you should give the **coefficients** and **intercept** of the linear regression **model**.","user":"anonymous","dateUpdated":"2017-11-15T19:51:09+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4f) Evaluate the linear regression</h3>\n<p>In the first cell, you need to evaluate the linear regression model by using the <strong>RegressionEvaluator</strong>. Calculate the <strong>r2</strong> and <strong>mae</strong> for the <strong>train_data_featured</strong>, <strong>val_data_featured</strong>, and <strong>test_data_featured</strong> dataframes.</p>\n<p>In the second cell, you should give the <strong>coefficients</strong> and <strong>intercept</strong> of the linear regression <strong>model</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316874_1528124111","id":"20171030-091725_352978389","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:51:09+0000","dateFinished":"2017-11-15T19:51:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:382"},{"text":"%pyspark\n# Evaluator\nevaluator = RegressionEvaluator(labelCol=\"sellingPrice\")\n\ntrain_r2 = evaluator.evaluate(train_predictions, {evaluator.metricName: \"r2\"})\ntrain_mae = evaluator.evaluate(train_predictions, {evaluator.metricName: \"mae\"})\n\nval_r2 = evaluator.evaluate(val_predictions, {evaluator.metricName: \"r2\"})\nval_mae = evaluator.evaluate(val_predictions, {evaluator.metricName: \"mae\"})\n\ntest_r2 = evaluator.evaluate(test_predictions, {evaluator.metricName: \"r2\"})\ntest_mae = evaluator.evaluate(test_predictions, {evaluator.metricName: \"mae\"})\n\nprint(\"Train R2 = %g \" % (train_r2))\nprint(\"Validation R2 = %g \" % (val_r2))\nprint(\"Test R2 = %g \" % (test_r2))\nprint(\"-----------------\")\nprint(\"Train MAE = %g \" % (train_mae))\nprint(\"Validation MAE = %g \" % (val_mae))\nprint(\"Test MAE = %g \" % (test_mae))","user":"anonymous","dateUpdated":"2017-12-14T15:28:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Train R2 = 0.493386 \nValidation R2 = 0.516609 \nTest R2 = -0.671476 \n-----------------\nTrain MAE = 7024.71 \nValidation MAE = 6705.15 \nTest MAE = 7186.27 \n"}]},"apps":[],"jobName":"paragraph_1509360316874_1528124111","id":"20171029-232815_1751863699","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:23+0000","dateFinished":"2017-12-14T15:28:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:383"},{"text":"%pyspark\n# Print the coefficients and intercept for linear regression\n\ncoefficients = model.coefficients\nintercept = model.intercept\n\nprint(\"Coefficients: %s\" % str(coefficients))\nprint(\"Intercept: %s\" % str(intercept))","user":"anonymous","dateUpdated":"2017-12-14T15:28:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Coefficients: [434.500590594,-1.61728991653,312.056211306,-0.132369529958,455.080740022]\nIntercept: -2488.60241956\n"}]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171029-232943_765751139","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:29+0000","dateFinished":"2017-12-14T15:28:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:384"},{"text":"%pyspark\n#TEST\nTest.assertEquals(round(train_r2, 2), 0.49, \"The train R2 should be around 0.43.\")\nTest.assertEquals(round(val_r2, 2), 0.52, \"The validation R2 should be around 0.56.\")\nTest.assertEquals(round(test_r2, 2), -0.67, \"The test R2 should be around 0.56.\")\n\nTest.assertEquals(round(train_mae, 0), 7025, \"The train mae should be around 6904.\")\nTest.assertEquals(round(val_mae, 0), 6705, \"The validation mae should be around 6352.\")\nTest.assertEquals(round(test_mae, 0), 7186, \"The test mae should be around 6743.\")\n\nTest.assertEquals([round(x, 2) for x in coefficients], [434.50,-1.62,312.06,-0.13,455.08], \"The coefficients of the linear regression are not correct.\")\nTest.assertEquals(round(intercept, 2), -2488.60, \"The intercept of the linear regression is not correct.\")","user":"anonymous","dateUpdated":"2017-12-14T15:28:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171030-092358_1624563392","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:32+0000","dateFinished":"2017-12-14T15:28:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:385"},{"text":"%md\n### (4g) Prepare data for logistic regression\nIn this part you need to prepare the **train_data**, **val_data**, and **test_data** by setting a **featureCol**.\n\nIn the logistic regression you need to predict the **bodyStyleIndex** based on the features cylinderCapacityValue, enginePowerValue, mileageValue, age, and sellingPrice. You need to combine cylinderCapacityValue, enginePowerValue, mileageValue, age, and sellingPrice into one featureCol called **features**. This needs to be done for the train_data, val_data, and test_data dataframes.","user":"anonymous","dateUpdated":"2017-11-15T19:52:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4g) Prepare data for logistic regression</h3>\n<p>In this part you need to prepare the <strong>train_data</strong>, <strong>val_data</strong>, and <strong>test_data</strong> by setting a <strong>featureCol</strong>.</p>\n<p>In the logistic regression you need to predict the <strong>bodyStyleIndex</strong> based on the features cylinderCapacityValue, enginePowerValue, mileageValue, age, and sellingPrice. You need to combine cylinderCapacityValue, enginePowerValue, mileageValue, age, and sellingPrice into one featureCol called <strong>features</strong>. This needs to be done for the train_data, val_data, and test_data dataframes.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171030-093951_581055312","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:52:02+0000","dateFinished":"2017-11-15T19:52:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:386"},{"text":"%pyspark\ntrain_data_logReg = (VectorAssembler(inputCols= [\"cylinderCapacityValue\", \"enginePowerValue\", \"mileageValue\", \"age\", \"sellingPrice\"], outputCol=\"features\")\n                    .transform(train_data))\n\nval_data_logReg = (VectorAssembler(inputCols= [\"cylinderCapacityValue\", \"enginePowerValue\", \"mileageValue\", \"age\", \"sellingPrice\"], outputCol=\"features\")\n                    .transform(val_data))\n\ntest_data_logReg = (VectorAssembler(inputCols= [\"cylinderCapacityValue\", \"enginePowerValue\", \"mileageValue\", \"age\", \"sellingPrice\"], outputCol=\"features\")\n                    .transform(test_data))\n","user":"anonymous","dateUpdated":"2017-12-14T15:28:38+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171029-233014_2067132423","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:38+0000","dateFinished":"2017-12-14T15:28:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:387"},{"text":"%pyspark\n# Test\nTest.assertEquals(train_data_logReg.columns[-1], \"features\", \"The final column of the train_data_logReg dataframe should be 'features'.\")\nTest.assertEquals(val_data_logReg.columns[-1], \"features\", \"The final column of the val_data_logReg dataframe should be 'features'.\")\nTest.assertEquals(test_data_logReg.columns[-1], \"features\", \"The final column of the test__data_logReg dataframe should be 'features'.\")\n\nTest.assertEquals(len(train_data_featured.select(\"features\").first()[\"features\"]), 5, \"The feature colomn in the train_data_logReg dataframe should be a vector of 5 columns.\")\nTest.assertEquals(len(val_data_featured.select(\"features\").first()[\"features\"]), 5, \"The feature colomn in the val_data_logReg dataframe should be a vector of 5 columns.\")\nTest.assertEquals(len(test_data_featured.select(\"features\").first()[\"features\"]), 5, \"The feature colomn in the val_data_logReg dataframe should be a vector of 5 columns.\")","user":"anonymous","dateUpdated":"2017-12-14T15:28:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171030-094403_761184660","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:41+0000","dateFinished":"2017-12-14T15:28:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:388"},{"text":"%md\n### (4h) Initialize a logistic regression model\n\nInitialize a **LogisticRegression** with labelCol = **bodyIndex** and featuresCol = **features**. Afterwards, **fit** the model on the **train_data_logReg** dataframe.\n\n> Note: The logistic regression model is not yet seen in class, but the logic to create a logReg is similar to the logic of all other ML models. For some more information, [click here](https://spark.apache.org/docs/latest/ml-classification-regression.html#multinomial-logistic-regression)","user":"anonymous","dateUpdated":"2017-12-02T12:16:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4h) Initialize a logistic regression model</h3>\n<p>Initialize a <strong>LogisticRegression</strong> with labelCol = <strong>bodyIndex</strong> and featuresCol = <strong>features</strong>. Afterwards, <strong>fit</strong> the model on the <strong>train_data_logReg</strong> dataframe.</p>\n<blockquote>\n  <p>Note: The logistic regression model is not yet seen in class, but the logic to create a logReg is similar to the logic of all other ML models. For some more information, <a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#multinomial-logistic-regression\">click here</a></p>\n</blockquote>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171030-094732_1077999122","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-02T12:16:35+0000","dateFinished":"2017-12-02T12:16:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:389"},{"text":"%pyspark\nLogReg = LogisticRegression(labelCol=\"bodyStyleIndex\", featuresCol=\"features\")\n\nmodelLogReg = LogReg.fit(train_data_logReg)","user":"anonymous","dateUpdated":"2017-12-14T15:28:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171029-233257_1490519065","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:44+0000","dateFinished":"2017-12-14T15:28:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:390"},{"text":"%pyspark\nTest.assertEquals(str(modelLogReg).split(\"_\")[0], 'LogisticRegression', \"The model should be a LogisticRegression object.\")","user":"anonymous","dateUpdated":"2017-12-14T15:28:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171030-104252_2065509224","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:51+0000","dateFinished":"2017-12-14T15:28:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:391"},{"text":"%md\n### (4i) Make predictions\n\nMake predictions for the **train_data_logReg**, **val_data_logReg**, and **test_data_logReg** dataframes.","user":"anonymous","dateUpdated":"2017-11-15T19:53:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4i) Make predictions</h3>\n<p>Make predictions for the <strong>train_data_logReg</strong>, <strong>val_data_logReg</strong>, and <strong>test_data_logReg</strong> dataframes.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316875_1527739362","id":"20171030-095130_553400274","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:53:24+0000","dateFinished":"2017-11-15T19:53:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:392"},{"text":"%pyspark\ntrain_data_logReg_pred = modelLogReg.transform(train_data_logReg)\nval_data_logReg_pred = modelLogReg.transform(val_data_logReg)\ntest_data_logReg_pred = modelLogReg.transform(test_data_logReg)\n","user":"anonymous","dateUpdated":"2017-12-14T15:28:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509360316876_1525815617","id":"20171029-233354_882709864","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:56+0000","dateFinished":"2017-12-14T15:28:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:393"},{"text":"%pyspark\n# TEST\nTest.assertEquals(train_data_logReg_pred.columns[-1], \"prediction\", \"The final column of the train_data_logReg_pred dataframe should be 'prediction'.\")\nTest.assertEquals(val_data_logReg_pred.columns[-1], \"prediction\", \"The final column of the val_data_logReg_pred dataframe should be 'prediction'.\")\nTest.assertEquals(test_data_logReg_pred.columns[-1], \"prediction\", \"The final column of the test_data_logReg_pred dataframe should be 'features'.\")","user":"anonymous","dateUpdated":"2017-12-14T15:28:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316876_1525815617","id":"20171030-095256_91113564","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:28:59+0000","dateFinished":"2017-12-14T15:28:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:394"},{"text":"%md\n### (4j) Evaluate the logistic regression\nIn the first cell, you need to evaluate the logistic regression model by using the **MulticlassClassificationEvaluator**. Calculate the **accuracy** for the **train_data_logReg_pred**, **val_data_logReg_pred**, and **test_data_logReg_pred** dataframes.\n\nIn the second cell, you should give the **coefficients** and **intercept** of the logistic regression.\n\n> Note the the logistic regression is unseen in class, but you can get inspiration for the evaluation process by looking at the procedure for decision trees and linear regressions.","user":"anonymous","dateUpdated":"2017-12-02T16:10:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(4j) Evaluate the logistic regression</h3>\n<p>In the first cell, you need to evaluate the logistic regression model by using the <strong>MulticlassClassificationEvaluator</strong>. Calculate the <strong>accuracy</strong> for the <strong>train_data_logReg_pred</strong>, <strong>val_data_logReg_pred</strong>, and <strong>test_data_logReg_pred</strong> dataframes.</p>\n<p>In the second cell, you should give the <strong>coefficients</strong> and <strong>intercept</strong> of the logistic regression.</p>\n<blockquote>\n  <p>Note the the logistic regression is unseen in class, but you can get inspiration for the evaluation process by looking at the procedure for decision trees and linear regressions.</p>\n</blockquote>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316876_1525815617","id":"20171030-095349_1007199209","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-02T16:10:40+0000","dateFinished":"2017-12-02T16:10:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:395"},{"text":"%pyspark\n# Evaluator\nlogreg_evaluator = MulticlassClassificationEvaluator().setLabelCol(\"bodyStyleIndex\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\");\n\ntrain_accuracy = logreg_evaluator.evaluate(train_data_logReg_pred)\nval_accuracy = logreg_evaluator.evaluate(val_data_logReg_pred)\ntest_accuracy = logreg_evaluator.evaluate(test_data_logReg_pred)\n\nprint(\"Train accuracy = %g \" % (train_accuracy))\nprint(\"Validation accuracy = %g \" % (val_accuracy))\nprint(\"Test accuracy = %g \" % (test_accuracy))","user":"anonymous","dateUpdated":"2017-12-14T15:29:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Train accuracy = 0.395882 \nValidation accuracy = 0.389927 \nTest accuracy = 0.408975 \n"}]},"apps":[],"jobName":"paragraph_1509360316876_1525815617","id":"20171029-233929_636420699","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:29:02+0000","dateFinished":"2017-12-14T15:29:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:396"},{"text":"%pyspark\ncoeff = modelLogReg.coefficientMatrix\ninterc = modelLogReg.interceptVector\n\nprint(\"Coefficients: \\n\" + str(coeff))\nprint(\"Intercept: \" + str(interc))\n\n\n","user":"anonymous","dateUpdated":"2017-12-14T15:29:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Coefficients: \nDenseMatrix([[ -8.42610745e-04,   1.01341587e-02,  -1.11372526e-06,\n                2.89618463e-02,  -6.72883445e-06],\n             [  9.79018321e-05,   3.17250431e-03,   9.20779662e-06,\n               -1.80198207e-01,   7.34203699e-06],\n             [  6.48957616e-04,   1.45297067e-04,   7.64805503e-06,\n               -1.46438863e-01,   2.74355897e-05],\n             [  1.83123939e-04,  -1.31484621e-02,   7.22171690e-06,\n               -8.56128167e-02,  -1.55686458e-06],\n             [ -3.61090968e-04,  -1.11033514e-02,  -1.33109998e-05,\n                8.50958529e-03,  -6.78523569e-05],\n             [ -8.73449128e-04,   1.97695163e-02,   4.62120876e-07,\n                1.33221237e-01,   2.76285410e-05],\n             [ -8.69949598e-04,   2.60402976e-02,   2.94437081e-06,\n                1.31402941e-01,   2.83833871e-05],\n             [  2.19119188e-03,  -3.28145435e-02,   8.61116094e-06,\n               -2.06488839e-01,  -3.88868761e-05],\n             [ -2.91531404e-04,  -1.29100924e-03,  -2.88926058e-06,\n                1.61286104e-01,   2.83066210e-05],\n             [  1.17456573e-04,  -9.04407784e-04,  -1.87812355e-05,\n                1.55357013e-01,  -4.07124372e-06]])\nIntercept: [3.26189489819,1.42865342705,0.115825277068,2.33802039829,4.35347064721,-1.2006445917,-2.48223076666,0.233803474782,-4.13605934436,-3.91273341987]\n"}]},"apps":[],"jobName":"paragraph_1509360316876_1525815617","id":"20171029-233530_1863795606","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:29:07+0000","dateFinished":"2017-12-14T15:29:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:397"},{"text":"%pyspark\n#TEST\nTest.assertEquals(round(train_accuracy, 2), 0.40, \"The train accuracy should be around 0.40.\")\nTest.assertEquals(round(val_accuracy, 2), 0.39, \"The validation accuracy should be around 0.39.\")\nTest.assertEquals(round(test_accuracy, 2), 0.40, \"The test accuracy should be around 0.40.\")\n\nTest.assertEquals(str(type(coeff)), \"<class 'pyspark.ml.linalg.DenseMatrix'>\", \"The coefficient matrix should be of type 'pyspark.ml.linalg.DenseMatrix'.\")\nTest.assertEquals([round(x, 2) for x in interc], [3.26, 1.43, 0.12, 2.34, 4.35, -1.20, -2.48, 0.23, -4.14, -3.91], \"The intercept vector of the linear regression is not correct.\")","user":"anonymous","dateUpdated":"2017-12-14T15:29:11+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1 test passed.\n1 test passed.\n1 test failed. The test accuracy should be around 0.40.\n1 test passed.\n1 test passed.\n"}]},"apps":[],"jobName":"paragraph_1509360316876_1525815617","id":"20171030-095941_1019566743","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:29:11+0000","dateFinished":"2017-12-14T15:29:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:398"},{"text":"%md \n## Part 5: Theoretical questions\nIn this final part some theoretical questions are asked. There are no detail questions, but your answers will give an indication whether or not you understand the core concepts of big data tools.\n\nBelow we will have a seperate cell for each question. You can give your answer in the same cell below the word **Answer:**.\n","user":"anonymous","dateUpdated":"2017-11-15T19:54:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 5: Theoretical questions</h2>\n<p>In this final part some theoretical questions are asked. There are no detail questions, but your answers will give an indication whether or not you understand the core concepts of big data tools.</p>\n<p>Below we will have a seperate cell for each question. You can give your answer in the same cell below the word <strong>Answer:</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316876_1525815617","id":"20171029-233846_474543455","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-11-15T19:54:52+0000","dateFinished":"2017-11-15T19:54:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:399"},{"text":"%md\n### (5a) Assuming we are working with realy large files on a large cluster of nodes, why is it benficial to use HDFS instead of working directly on the local file?\n\n#### Answer:  \n1.Reliability and accessibility \nIn a local file system, data is maintained in one system hence data is only accessible in that particular local data node. If the machine is down, we will not be able to access the data. Failover chance is high. With HDFS, data is replicated in different nodes using a master/slave architecture in which client will still be able to read the data even if one of the nodes has failed. It has high fault tolerance that can detect faults and apply quick and automatic recovery. \n \n2.Scalable and speed \nHDFS is able to handle large volume in as it is horizontally scalable as compared to relying only on one local file system. It is able to access information faster and can serve a large number of clients by simply adding more machines to the cluster.  ","user":"anonymous","dateUpdated":"2017-12-14T15:29:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(5a) Assuming we are working with realy large files on a large cluster of nodes, why is it benficial to use HDFS instead of working directly on the local file?</h3>\n<h4>Answer:</h4>\n<p>1.Reliability and accessibility<br/>In a local file system, data is maintained in one system hence data is only accessible in that particular local data node. If the machine is down, we will not be able to access the data. Failover chance is high. With HDFS, data is replicated in different nodes using a master/slave architecture in which client will still be able to read the data even if one of the nodes has failed. It has high fault tolerance that can detect faults and apply quick and automatic recovery. </p>\n<p>2.Scalable and speed<br/>HDFS is able to handle large volume in as it is horizontally scalable as compared to relying only on one local file system. It is able to access information faster and can serve a large number of clients by simply adding more machines to the cluster.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316877_1525430869","id":"20171030-081040_60439560","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:29:39+0000","dateFinished":"2017-12-14T15:29:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:400"},{"text":"%md\n### (5b) Why should I choose to work with the Spark framework instead of the MapReduce framework?\n\n\n#### Answer:\nThe advantages of using Spark instead of MapReduce are speed and ease of use. \n\n1.Speed:  \nSpark handles most of its operations in memory by copying them from the distributed physical storage into logical RAM memory. It is faster than using MapReduce (a group of map tasks and reduce tasks) which write and read all the data back to and from the physical storage medium after each operation and this is considered to be I/O intensive. \nSpark's functionality for handling advanced data processing tasks such as real time stream processing and machine learning is way ahead of what is possible with MapReduce.  \n \n2.Ease of use: \nSpark is easy to use compared to MapReduce. The same thing can be done using Spark with only a few lines of code but would require over 100 lines of codes using MapReduce which makes sparks more versatile.  \nWe can use scala, python, java or R API's in Spark. However, we can only use Java or Python if we use MapReduce. \nSpark inherits a number of functionalities of API and complements with parallelized Spark functionalities. However, we have to write complicated Map Reduce jobs if we use MapReduce \nIn terms of execution model, MapReduce is great for batch processing but Spark offers the option to operate not only by batch processing, it also allows real-time processing such as streaming. \n \nHowever, as a lot of memory and CPU are needed for Spark, it may be more costly than using MapReduce. ","user":"anonymous","dateUpdated":"2017-12-14T15:30:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(5b) Why should I choose to work with the Spark framework instead of the MapReduce framework?</h3>\n<h4>Answer:</h4>\n<p>The advantages of using Spark instead of MapReduce are speed and ease of use. </p>\n<p>1.Speed:<br/>Spark handles most of its operations in memory by copying them from the distributed physical storage into logical RAM memory. It is faster than using MapReduce (a group of map tasks and reduce tasks) which write and read all the data back to and from the physical storage medium after each operation and this is considered to be I/O intensive.<br/>Spark&rsquo;s functionality for handling advanced data processing tasks such as real time stream processing and machine learning is way ahead of what is possible with MapReduce. </p>\n<p>2.Ease of use:<br/>Spark is easy to use compared to MapReduce. The same thing can be done using Spark with only a few lines of code but would require over 100 lines of codes using MapReduce which makes sparks more versatile.<br/>We can use scala, python, java or R API&rsquo;s in Spark. However, we can only use Java or Python if we use MapReduce.<br/>Spark inherits a number of functionalities of API and complements with parallelized Spark functionalities. However, we have to write complicated Map Reduce jobs if we use MapReduce<br/>In terms of execution model, MapReduce is great for batch processing but Spark offers the option to operate not only by batch processing, it also allows real-time processing such as streaming. </p>\n<p>However, as a lot of memory and CPU are needed for Spark, it may be more costly than using MapReduce.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316877_1525430869","id":"20171030-081101_1652798071","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:30:02+0000","dateFinished":"2017-12-14T15:30:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:401"},{"text":"%md\n### (5c) I am a PhD student with good math skills. Currently I am working hard on developping a new algorithms. Which tool(s) should I use? Could you give me an answer together with an overview of pros and cons of different ML frameworks?\n\n#### Answer:\nTheano, Torch or Tensorflow are tools that would enable you to go mathematically deep to design and develop algorithms using deep learning framework. Each has its pros and cons and it depends on your ultimate goal.\n\n1.Theano\n\nPros:\na.Has extensive volume of tutorials and documentations for reference.\nb.Fast execution speed\nc.Most new ideas can be implemented quickly with simple modifications of existing layers.\nd.Good debugging\n\nCons:\na.Multi-GPU is not by default\nb.Quantity and quality of pretrained models is declining\nc.Run-time and memory competitive\n\n2.Torch\n\nPros:\na.Uses multi-GPU by default\nb.It is designed to be portable, fast, extensible and easy to use in development.\nc.Easy to set up\nd.Has large number of community-contributed packages, blogs and supporting documents which gives Torch users a versatile range of support and functionality\ne.Numerous models of good quality are available.\nf.Fast performance\n\nCons:\na.Rely all on cuDNN. Even though it is as fast as others, it is always memory hungry.\nb.Requires LuaJIT to run models. It is not mainstream and does cause integration issues.\n\n3.Tensorflow\n\nPros:\na.Uses multi-GPU by default\nb.It enables model parallelization. It is able to do partial subgraph computation which involves taking a subsample of the total neural network and training it apart from the rest of the network. It allows distributed training.\nc.Has a good amount of documentation for installation to make it easier to set up and run. Learning materials or tutorials are available to help beginners understand neural networks better.\nd.Highly visual and it is good for production\n\nCons:\na.Though speed is as fast as Theano and Torch, it is memory hungry.\nb.Does not have many pretrained models\nc.Due to its fast growth, tutorials may be outdated.\n\nWhich tool should you use?\nIt has been announced that Theano development have been put to an end where they will stop actively implementing new features and they will only do minimal maintenance to keep it working for a year. Both Torch and Tensorflow have good performance however Tensorflow may be a more preferred choice as it can be run on Python and compiled in C++ which is easier for users who already know those languages than trying to use Torch which requires LuaJIT, a compiler for Lua programming language. Other than this, there is also high development growth in Tensorflow which may be able to better support your future developments in machine learning.","user":"anonymous","dateUpdated":"2017-12-14T15:30:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>(5c) I am a PhD student with good math skills. Currently I am working hard on developping a new algorithms. Which tool(s) should I use? Could you give me an answer together with an overview of pros and cons of different ML frameworks?</h3>\n<h4>Answer:</h4>\n<p>Theano, Torch or Tensorflow are tools that would enable you to go mathematically deep to design and develop algorithms using deep learning framework. Each has its pros and cons and it depends on your ultimate goal.</p>\n<p>1.Theano</p>\n<p>Pros:<br/>a.Has extensive volume of tutorials and documentations for reference.<br/>b.Fast execution speed<br/>c.Most new ideas can be implemented quickly with simple modifications of existing layers.<br/>d.Good debugging</p>\n<p>Cons:<br/>a.Multi-GPU is not by default<br/>b.Quantity and quality of pretrained models is declining<br/>c.Run-time and memory competitive</p>\n<p>2.Torch</p>\n<p>Pros:<br/>a.Uses multi-GPU by default<br/>b.It is designed to be portable, fast, extensible and easy to use in development.<br/>c.Easy to set up<br/>d.Has large number of community-contributed packages, blogs and supporting documents which gives Torch users a versatile range of support and functionality<br/>e.Numerous models of good quality are available.<br/>f.Fast performance</p>\n<p>Cons:<br/>a.Rely all on cuDNN. Even though it is as fast as others, it is always memory hungry.<br/>b.Requires LuaJIT to run models. It is not mainstream and does cause integration issues.</p>\n<p>3.Tensorflow</p>\n<p>Pros:<br/>a.Uses multi-GPU by default<br/>b.It enables model parallelization. It is able to do partial subgraph computation which involves taking a subsample of the total neural network and training it apart from the rest of the network. It allows distributed training.<br/>c.Has a good amount of documentation for installation to make it easier to set up and run. Learning materials or tutorials are available to help beginners understand neural networks better.<br/>d.Highly visual and it is good for production</p>\n<p>Cons:<br/>a.Though speed is as fast as Theano and Torch, it is memory hungry.<br/>b.Does not have many pretrained models<br/>c.Due to its fast growth, tutorials may be outdated.</p>\n<p>Which tool should you use?<br/>It has been announced that Theano development have been put to an end where they will stop actively implementing new features and they will only do minimal maintenance to keep it working for a year. Both Torch and Tensorflow have good performance however Tensorflow may be a more preferred choice as it can be run on Python and compiled in C++ which is easier for users who already know those languages than trying to use Torch which requires LuaJIT, a compiler for Lua programming language. Other than this, there is also high development growth in Tensorflow which may be able to better support your future developments in machine learning.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1509360316877_1525430869","id":"20171030-081351_1045875615","dateCreated":"2017-10-30T10:45:16+0000","dateStarted":"2017-12-14T15:30:26+0000","dateFinished":"2017-12-14T15:30:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:402"},{"text":"%md\n","dateUpdated":"2017-10-30T10:52:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509360316877_1525430869","id":"20171030-081645_200638874","dateCreated":"2017-10-30T10:45:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:403"}],"name":"Assignment/Individual Assignment","id":"2CW7UG1WD","angularObjects":{"2CUY2SPYF:shared_process":[],"2CX8HYUW5:shared_process":[],"2CVWSTEJX:shared_process":[],"2CU7YVEEU:shared_process":[],"2CVTTTEEJ:shared_process":[],"2CVMK2FSD:shared_process":[],"2CUQGF2H7:shared_process":[],"2CX8NXVEG:shared_process":[],"2CWGP6SAY:shared_process":[],"2CVSHPEMV:shared_process":[],"2CUUZK5C4:shared_process":[],"2CUHNPV2B:shared_process":[],"2CWP2SYTR:shared_process":[],"2CVTV38BV:shared_process":[],"2CVY2C5SC:shared_process":[],"2CVCPP8ZK:shared_process":[],"2CXCAR5P9:shared_process":[],"2CUTDXXD7:shared_process":[],"2CXBMS9S5:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}